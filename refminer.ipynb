{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1476行目の下に以下を追加:  \n",
    "ids = IndexSearch(MAIN_DIR,'abstractL') ids.create_input_textfile(os.path.join(INPUT_DIR,\"abstractL_input.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "エラーのため421行目「abst.decode('utf-8')」を「abst」に変更. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import re, os, sys, glob, shutil, sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WS_DIR = '/Users/petadimensionlab/ws/apps/refminer/'\n",
    "RESOURCE_DIR = os.path.join(WS_DIR,'resources')\n",
    "MALLET_DIR = '/Users/petadimensionlab/ws/apps/mallet-2.0.8/bin/mallet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "THEME = 'PID'\n",
    "MAIN_DIR = os.path.join(WS_DIR,THEME)\n",
    "STAT_DATA_DIR = os.path.join(MAIN_DIR,'stat')\n",
    "FTS_BASE_DIR = os.path.join(MAIN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PUBMED_DIR = os.path.join(WS_DIR,'Pubmed')\n",
    "sys.path.append(PUBMED_DIR)\n",
    "from Pubmed_processing import Pubmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FilReter_DIR = os.path.join(WS_DIR,'FilReter')\n",
    "DB_BASE_DIR = os.path.join(RESOURCE_DIR,'db')\n",
    "sys.path.append(FilReter_DIR)\n",
    "from FilReter import FullTextSearch, DictSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MISC_BASE_DIR = os.path.join(DB_BASE_DIR,'misc')\n",
    "sys.path.append(MISC_BASE_DIR)\n",
    "#from AhoCorasick import AhoCorasick\n",
    "#from Inflector import Inflector\n",
    "from nltk import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from whoosh.index import create_in, open_dir\n",
    "from whoosh.fields import *\n",
    "from whoosh.query import *\n",
    "from whoosh import qparser\n",
    "from whoosh.qparser import QueryParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class IndexSearch(FullTextSearch): #\n",
    "    def __init__(self,FTS_BASE_DIR,source='abstract'):\n",
    "        self.FTS_BASE_DIR = FTS_BASE_DIR\n",
    "        self.DOCS_IDXDIR = os.path.join(self.FTS_BASE_DIR,source+'_idxDB')\n",
    "        self.read_file_name = os.path.join(MAIN_DIR,source+'.pkl')\n",
    "    \"\"\" save all documents as cPickle (unicode) format \"\"\"\n",
    "    def pickle_docs(self,pmid_doc,source):\n",
    "        import codecs\n",
    "        import pickle as pickle\n",
    "        save_file_name = os.path.join(self.FTS_BASE_DIR,source+'.pkl')\n",
    "        fw = open(save_file_name,'wb')\n",
    "        try:\n",
    "            pickle.dump(pmid_doc,fw,protocol=2)#protocol=pickle.HIGHEST_PROTOCOL\n",
    "        except:\n",
    "            print ('Failed to save.')\n",
    "    \"\"\" create an input text-file from a pickled file \"\"\"\n",
    "    def create_input_textfile(self,input_file):\n",
    "        import pickle as pickle\n",
    "        pf = open(self.read_file_name,'rb')\n",
    "        pmid_abst = pickle.load(pf)\n",
    "        fw = open(input_file,'w')\n",
    "        label = 'NA'\n",
    "        for pmid, abst in pmid_abst.items():\n",
    "            fw.write(pmid+' '+label+' '+abst+'\\n') #.decode('utf-8')\n",
    "        fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AbstractIndexSearch(IndexSearch):\n",
    "    def __init__(self,FTS_BASE_DIR,source='abstract'):\n",
    "        self.FTS_BASE_DIR = FTS_BASE_DIR\n",
    "        self.DOCS_IDXDIR = os.path.join(self.FTS_BASE_DIR,source+'_idxDB')\n",
    "    \"\"\" create index for abstract \"\"\"    \n",
    "    def create_indexdb(self,source='abstract'):\n",
    "        self.DOCS_IDXDIR = os.path.join(self.FTS_BASE_DIR,source+'_idxDB')\n",
    "        import glob\n",
    "        import pickle as pickle\n",
    "        if not os.path.exists(self.DOCS_IDXDIR):\n",
    "            os.mkdir(self.DOCS_IDXDIR)\n",
    "        ### create an index database ###\n",
    "        schema = Schema(title=TEXT(stored=True), path=ID(stored=True), content=TEXT(stored=True))\n",
    "        ix = create_in(self.DOCS_IDXDIR, schema)\n",
    "        writer = ix.writer()\n",
    "        pf = open(os.path.join(self.FTS_BASE_DIR,source+'.pkl'),'rb')\n",
    "        pmid_doc = pickle.load(pf) \n",
    "        for pmid, doc in pmid_doc.items():\n",
    "            writer.add_document(title=pmid, path=pmid, content=doc)\n",
    "        os.chdir(self.FTS_BASE_DIR)\n",
    "        try:\n",
    "            writer.commit(optimize=True)\n",
    "        except:\n",
    "            print (\"Failed to write index.\")\n",
    "            writer.cancel()\n",
    "        ix.close()\n",
    "    \"\"\" create a list of word-frequency \"\"\"\n",
    "    def create_VocabDB(self,db_name='Vocab',lemmatization='False'):\n",
    "        if lemmatization=='False':\n",
    "            pm = Pubmed(THEME)\n",
    "            pm.calc_vocab_freq()\n",
    "            ds = DictSearch(db_name)\n",
    "            ds.create_simstringdb(db_name,os.path.join(STAT_DATA_DIR,db_name+'.txt'))\n",
    "        else:\n",
    "            fr = open(os.path.join(STAT_DATA_DIR,self.db_name+'.txt')).readlines()\n",
    "            word_lst = []\n",
    "            for word in fr:\n",
    "                word = word.replace('\\n','')\n",
    "                word_lst.append(word)\n",
    "            sew = self.get_stem_equivalent_pair(db_name=db_name)\n",
    "            fw = open(os.path.join(STAT_DATA_DIR,db_name+'L.txt'),'w')\n",
    "            for word in word_lst:\n",
    "                try:\n",
    "                    pre_word = word\n",
    "                    word = sew[word]\n",
    "                    fw.write(word+'\\n')\n",
    "                except:\n",
    "                    fw.write(word+'\\n')\n",
    "            fw.close()\n",
    "            vocab_dict_name = THEME+'Lvocabdist'\n",
    "            ds = DictSearch(vocab_dict_name)\n",
    "            ds.create_simstringdb(db_name+'L',os.path.join(STAT_DATA_DIR,db_name+'L.txt')) \n",
    "    \"\"\" get a conversion list of greek unicode to symbol \"\"\"\n",
    "    def greek_symbol2name(self):\n",
    "        fr = open(os.path.join(RESOURCE_DIR,'dict/greek_unicode_utf8literal_symbol.txt')).readlines()\n",
    "        s2n = {}\n",
    "        for line in fr:\n",
    "            line = line.replace('\\n','')\n",
    "            uni, sym, utf8, name = line.split('\\t')\n",
    "            s2n[sym] = name\n",
    "        return s2n    \n",
    "    \"\"\" get a pair of stem equivalent words \"\"\"\n",
    "    def get_stem_equivalent_pair(self,db_name='Vocab',save2file='False'):\n",
    "        ds = DictSearch(db_name)\n",
    "        ps = PorterStemmer() \n",
    "        fr = open(os.path.join(STAT_DATA_DIR,db_name+'.txt')).readlines()\n",
    "        word_lst = []\n",
    "        for word in fr:\n",
    "            word = word.replace('\\n','')\n",
    "            word_lst.append(word)\n",
    "        sim_lst = []\n",
    "        for item in word_lst:\n",
    "            #db.threshold = 1.0-1.0/(len(item)+1) # theshold to obtain a unique return\n",
    "            #print( item )\n",
    "            res = ds.single_db_retrieve(item,threshold=0.8)\n",
    "            n = len(res)\n",
    "            if n==2:\n",
    "                sim_lst.append(res)\n",
    "            elif n>2:\n",
    "                head = res[0]\n",
    "                for i in range(1,n):\n",
    "                    sim_lst.append([res[0],res[i]])\n",
    "        ## get stem equivalent words ##\n",
    "        common_sfx_lst = []\n",
    "        fr = open(os.path.join(RESOURCE_DIR,'dict/common_suffix.txt')).readlines()\n",
    "        for item in fr:\n",
    "            item = item.replace('\\n','')\n",
    "            common_sfx_lst.append(item)\n",
    "        sew = {}\n",
    "        for lst in sim_lst:\n",
    "            smw0 = ps.stem(str(lst[0])) #len(str(lst[0]))-1\n",
    "            smw1 = ps.stem(str(lst[1]))\n",
    "            if smw0==smw1: # stem-equivalent\n",
    "                if len(str(lst[0]))>=len(str(lst[1])):\n",
    "                    sfx = str(lst[0]).replace(str(lst[1]),'')\n",
    "                    if sfx in common_sfx_lst:\n",
    "                        sew[lst[0]] = lst[1]\n",
    "                else:\n",
    "                    sfx = str(lst[1]).replace(str(lst[0]),'')\n",
    "                    if sfx in common_sfx_lst:\n",
    "                        sew[lst[1]] = lst[0]\n",
    "        if not save2file=='False':\n",
    "            fw = open(os.path.join(STAT_DATA_DIR,self.header+'_stem_equivalence.txt'),'w')\n",
    "            for key, val in sew.items():\n",
    "                fw.write(key+'\\t'+val+'\\n')\n",
    "            fw.close()\n",
    "        return sew\n",
    "    \"\"\" lemmatize documents \"\"\"\n",
    "    def lemmatize_docs(self,source='abstract'):\n",
    "        import pickle as pickle\n",
    "        vocab_name = THEME+'Vocab.txt'\n",
    "        vocab_db_name = THEME+'Vocab'\n",
    "        #if not os.path.exists(os.path.join(STAT_DATA_DIR,vocab_name)):\n",
    "        self.create_VocabDB(db_name=vocab_db_name,lemmatization='False')\n",
    "        pm = Pubmed(THEME)\n",
    "        #import pdb;pdb.set_trace()\n",
    "        pmid_doc = pm.collect_info('abstract')\n",
    "        gs2n = self.greek_symbol2name()\n",
    "        symbol_lst = gs2n.keys()\n",
    "        ## delete NO-abstract documents && relpace greek-words ##\n",
    "        for pmid, doc in pmid_doc.copy().items():\n",
    "            if doc == 'No abstract':\n",
    "                del pmid_doc[pmid]\n",
    "            else:\n",
    "                for sym in symbol_lst:\n",
    "                    match = doc.find(sym)\n",
    "                    if match>=0:\n",
    "                        doc.replace(sym,gs2n[sym]) #sym.decode('utf-8')\n",
    "        if not os.path.exists(self.DOCS_IDXDIR):\n",
    "            self.pickle_docs(pmid_doc,source)\n",
    "            self.create_indexdb(source=source)\n",
    "        fts = AbstractIndexSearch(MAIN_DIR,source)\n",
    "        sew = self.get_stem_equivalent_pair(db_name=vocab_db_name)\n",
    "        search_words = sew.keys()\n",
    "        for word in search_words:\n",
    "            pmids = fts.matched_docs(word)\n",
    "            for pmid in pmids:\n",
    "                try:\n",
    "                    doc = pmid_doc[pmid]\n",
    "                    Ldoc = doc.replace(word,sew[word])\n",
    "                    pmid_doc[pmid] = Ldoc #unicode(new_abst)\n",
    "                except:\n",
    "                    pass\n",
    "        self.pickle_docs(pmid_doc,source+'L')\n",
    "        self.create_indexdb(source=source+'L')\n",
    "    \"\"\" send a query to reference DB \"\"\"\n",
    "    def query2refdb(self,qtype,query,target='all'):\n",
    "        refdb_name = THEME+'Ref.db'\n",
    "        connection = sqlite3.connect(os.path.join(SAVE_DATA_DIR,refdb_name))\n",
    "        cursor = connection.cursor()\n",
    "        \"\"\" pubmed_id, pub_type, title, authors, journal, year, vol, issue, pages, abstract \"\"\"\n",
    "        sql = \"SELECT %s FROM tbl WHERE %s='%s';\" % (target,qtype,query)\n",
    "        cc = cursor.execute(sql)\n",
    "        tbl = []\n",
    "        for row in cc:\n",
    "            tbl.append(row[0])\n",
    "        cursor.close()\n",
    "        return tbl[0]\n",
    "    \"\"\" check the correponding abstract in which the retrieved word appears \"\"\"\n",
    "    def check_abst(self,querystring,search_field='content',qlim=5,write_as_html='True'):\n",
    "        flag = 0\n",
    "        pmid_lst = self.matched_docs(querystring=querystring,search_field=search_field,qlim=qlim)\n",
    "        if len(pmid_lst)==0:\n",
    "            flag = 1\n",
    "        tagged_query = '<font color=\"red\"><b>'+querystring+'</b></font>'\n",
    "        pmid_abst = {}\n",
    "        for pmid in pmid_lst:\n",
    "            title = self.query2refdb(qtype='pubmed_id',query=pmid,target='title')\n",
    "            pub_type = self.query2refdb(qtype='pubmed_id',query=pmid,target='pub_type')\n",
    "            journal = self.query2refdb(qtype='pubmed_id',query=pmid,target='journal')\n",
    "            year = self.query2refdb(qtype='pubmed_id',query=pmid,target='year')\n",
    "            abst = self.query2refdb(qtype='pubmed_id',query=pmid,target='abstract')\n",
    "            abst = abst.replace(querystring,tagged_query)\n",
    "            ref = '|'.join([title,journal,str(year),pub_type,abst])\n",
    "            pmid_abst[pmid] = ref\n",
    "        if write_as_html=='True':\n",
    "            if flag==0:\n",
    "                save_file_name = os.path.join(os.path.join(MAIN_DIR,'abst_check'),querystring+'.html')\n",
    "            else:\n",
    "                save_file_name = os.path.join(os.path.join(MAIN_DIR,'abst_check/noabst'),querystring+'.html')\n",
    "            fw = open(save_file_name,'w')\n",
    "            fw.write('<html><body>'+'\\n')\n",
    "            for pmid, ref in pmid_abst.items():\n",
    "                title,journal,year,pub_type,abst = ref.split('|')\n",
    "                fw.write('<h3>'+title+'</h3>'+'\\n')\n",
    "                fw.write('<h4><it>'+journal+'</it> ('+year+') ['+pub_type+'] [pmid: <a href=\"http://www.ncbi.nlm.nih.gov/pubmed?term='+pmid+'\" target=blank>'+pmid+'</a>]</h4>'+'\\n')\n",
    "                fw.write('<p>'+abst+'</p>'+'\\n')\n",
    "            fw.write('</body></html>')\n",
    "            fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle as pickle\n",
    "class LDA():\n",
    "    \"\"\" Latent Dirichlet Allocation by MALLET \"\"\"\n",
    "    def __init__(self,source='abstract'):\n",
    "        self.LDA_DIR = os.path.join(MAIN_DIR,'LDA')\n",
    "        self.header = THEME\n",
    "        self.source = source\n",
    "        self.MALLET = os.path.join(MALLET_DIR)\n",
    "        self.input_file = os.path.join(MAIN_DIR,self.source+'_input.txt')\n",
    "        self.train_file = os.path.join(self.LDA_DIR,self.header+self.source+'.mallet')\n",
    "        self.stopwords_file = os.path.join(os.path.join(RESOURCE_DIR,'stopwords'),'stopwords_all.txt')\n",
    "        self.ostate_zip = os.path.join(self.LDA_DIR,self.header+self.source+'_state.gz')\n",
    "        self.ostate = os.path.join(self.LDA_DIR,self.header+self.source+'_state.txt')\n",
    "        #odoc = os.path.join(LDA_OUTPUT_DIR,self.header+'_docs.txt')\n",
    "        self.okey = os.path.join(self.LDA_DIR,self.header+self.source+'_keys.txt')\n",
    "        #self.nodesfreqint = os.path.join(self.LDA_DIR,self.header+self.source+'_nodesfreqint.txt')\n",
    "    \"\"\" execute LDA \"\"\"        \n",
    "    def exec_LDA(self,num=-1,parms={'alpha':1.0,'beta':0.1,'num-topics':50,'num-threads':8,'num-top-words':20,'num-iterations':1000,'train':'on'}):\n",
    "        import gzip\n",
    "        #if not os.path.exists(self.input_file):\n",
    "        #    adis = IndexSearch(MAIN_DIR,self.source)\n",
    "        #    adis.create_input_textfile(input_file=self.input_file)\n",
    "        if parms['train']=='on':\n",
    "            exe1 = '%s import-file --input %s --output %s --keep-sequence --preserve-case --print-output FALSE --stoplist-file %s' % (self.MALLET, self.input_file, self.train_file, self.stopwords_file)\n",
    "            #print(exe1)\n",
    "            os.system(exe1)\n",
    "        if not num==-1:\n",
    "            self.okey = os.path.join(self.LDA_DIR,self.header+self.source+'_keys'+str(num)+'.txt')\n",
    "        exe2 = '%s train-topics --input %s --output-state %s --output-topic-keys %s --alpha %f --beta %f --num-topics %d --num-threads %d --num-top-words %d --num-iterations %d' % (self.MALLET,self.train_file,self.ostate_zip,self.okey,parms['alpha'],parms['beta'],parms['num-topics'],parms['num-threads'],parms['num-top-words'],parms['num-iterations'])\n",
    "        #print(exe2)\n",
    "        os.system(exe2)\n",
    "        fr = open(self.input_file, 'rb').readlines()\n",
    "        pmid_lst = []\n",
    "        for line in fr:\n",
    "            lst = line.decode().split(' ')\n",
    "            pmid_lst.append(lst[0])\n",
    "        f = gzip.open(self.ostate_zip, 'rb')\n",
    "        file_content = f.readlines()\n",
    "        f.close()\n",
    "        fr = []\n",
    "        for i in range(3,len(file_content)):\n",
    "            fr.append( file_content[i].decode('utf-8') )\n",
    "        #fr = file_content[3:len(file_content)]        \n",
    "        docnum = 0\n",
    "        fw = open(self.ostate,'w')\n",
    "        for line in fr:\n",
    "            lst = line.split(' ')\n",
    "            pmid = lst[1]\n",
    "            word = lst[4]\n",
    "            topicnum = lst[5]\n",
    "            if lst[0] == str(docnum):\n",
    "                pmid = pmid_lst[docnum]\n",
    "                fw.write(pmid+'\\n')\n",
    "                docnum += 1\n",
    "            fw.write(word+'\\t'+topicnum)\n",
    "        fw.close()\n",
    "        os.remove(self.ostate_zip)\n",
    "        os.remove(self.train_file)\n",
    "    \"\"\" return a reverse combined key \"\"\"\n",
    "    def get_reversekey(self,fkey):\n",
    "        lst = fkey.split('__')\n",
    "        rkey = lst[1]+'__'+lst[0]\n",
    "        return rkey\n",
    "    \"\"\" check whether a key exists or not \"\"\"\n",
    "    def is_keyexists(self,fkey,dic):\n",
    "        flag = 0\n",
    "        chk = dic.get(fkey,'NotExist')\n",
    "        if chk=='NotExist': # no-fkey\n",
    "            rkey = self.get_reversekey(fkey)\n",
    "            chk = dic.get(rkey,'NotExist')\n",
    "            if chk=='NotExist': # no rkey (and fkey)\n",
    "                dic[fkey] = 1\n",
    "            else: # rkey exists\n",
    "                val = dic[rkey]\n",
    "                val += 1\n",
    "                dic[rkey] = val\n",
    "        else: # fkey exists\n",
    "            val = dic[fkey]\n",
    "            val += 1\n",
    "            dic[fkey] = val\n",
    "    \"\"\" remove self-referred node \"\"\"\n",
    "    def remove_selfref(self,dic):\n",
    "        remkeys = []\n",
    "        for key in dic.keys():\n",
    "            key1, key2 = key.split('__')\n",
    "            if key1==key2:\n",
    "                remkeys.append(key)\n",
    "        for remkey in remkeys:\n",
    "            del dic[remkey]\n",
    "        return dic\n",
    "    \"\"\" get all possible pairs from a list \"\"\"\n",
    "    def gen_fkeys(self,wlst):\n",
    "        n = len(wlst)\n",
    "        fkey_lst = []\n",
    "        for i in range(n):\n",
    "            for j in range(i,n):\n",
    "                fkey = wlst[i]+'__'+wlst[j]\n",
    "                fkey_lst.append(fkey)\n",
    "        return fkey_lst\n",
    "    \"\"\" convert a pickle file to readable file format \"\"\"\n",
    "    def conv_pickle2txt(self,treeid):\n",
    "        pf =  open(os.path.join(self.LDA_DIR,self.header+self.source+'_com_'+treeid+'.pkl'),'rb')\n",
    "        nodes_freq = pickle.load(pf)\n",
    "        save_file_name = os.path.join(self.LDA_DIR,self.header+self.source+'_com_'+treeid+'.txt')\n",
    "        fw = open(save_file_name,'w')\n",
    "        for key, val in sorted(nodes_freq.items(), key=lambda x:x[1], reverse=True):\n",
    "            lst = key.split('__')\n",
    "            fw.write('\\t'.join(lst)+'\\t'+str(val)+'\\n')\n",
    "        fw.close()\n",
    "    \"\"\" convert a pickle file to readable file format \"\"\"\n",
    "    def conv_txt2pickle(self,treeid):\n",
    "        fr =  open(os.path.join(self.LDA_DIR,self.header+self.source+'_com_'+treeid+'.txt')).readlines()\n",
    "        nodes_freq = {}\n",
    "        for line in fr:\n",
    "            node1, node2, freq = line.split('\\t')\n",
    "            key = node1+'__'+node2\n",
    "            nodes_freq[key] = int(freq)\n",
    "        save_file_name = os.path.join(self.LDA_DIR,self.header+self.source+'_com_'+treeid+'.pkl')\n",
    "        fw = open(save_file_name,'w')\n",
    "        pickle.dump(nodes_freq,fw)\n",
    "        fw.close()\n",
    "    \"\"\" check convergence (not convergent if there exists a key with the freqency less than a given threshold) \"\"\"\n",
    "    def check_convergence(self,nodes_freq,thr=1):\n",
    "        flag = 0\n",
    "        freq_lst = [count for count in nodes_freq.values() if count<=thr]\n",
    "        if len(fre_lst)>0:\n",
    "            flag = 1\n",
    "        return flag\n",
    "    \"\"\" get edge counts \"\"\"\n",
    "    def get_nodes_counts(self,nodes_freq,save2file='False'):\n",
    "        fr = open(self.okey).readlines()\n",
    "        for line in fr:\n",
    "            line = line.replace('\\n','')\n",
    "            num1, num2, words = line.split('\\t')\n",
    "            wlst = words.split(' ')\n",
    "            nullidx = wlst.index('')\n",
    "            wlst.pop(nullidx)\n",
    "            fkey_lst = self.gen_fkeys(wlst)\n",
    "            for fkey in fkey_lst:\n",
    "                self.is_keyexists(fkey,nodes_freq)\n",
    "        nodes_freq = self.remove_selfref(nodes_freq)\n",
    "        if not save2file=='False':\n",
    "            save_file_name = os.path.join(self.LDA_DIR,self.header+self.source+'_com_d0.pkl')\n",
    "            fw = open(save_file_name,'wb')\n",
    "            pickle.dump(nodes_freq,fw)\n",
    "            fw.close()\n",
    "            self.conv_pickle2txt('d0')            \n",
    "        return nodes_freq\n",
    "    \"\"\" sequencial LDA \"\"\"\n",
    "    def seq_LDA(self,iter=10,convergence_check='False'):\n",
    "        nodes_freq = {}\n",
    "        for i in range(1,iter+1):\n",
    "            self.exec_LDA(num=i)\n",
    "            nodes_freq = self.get_nodes_counts(nodes_freq=nodes_freq,save2file='True')\n",
    "        if not convergence_check=='False':\n",
    "            chk = self.check_convergence(nodes_freq,thr=1)\n",
    "            count = itr\n",
    "            while chk==1:\n",
    "                nodes_freq = self.get_nodes_counts(nodes_freq=nodes_freq,save2file='True')\n",
    "                chk = self.check_convergence(nodes_freq,thr=1)\n",
    "                count += 1\n",
    "            print('convergence after %d computations.' % (count))\n",
    "    \"\"\" get nodes and their weight as lines \"\"\"\n",
    "    def conv2edge_weight_list(self,nodes_freq,lthr=1,hthr='infty'):\n",
    "        wedge_lst = []\n",
    "        for key, freq in nodes_freq.items():\n",
    "            node1, node2 = key.split('__')\n",
    "            if hthr=='infty':\n",
    "                if freq>=lthr:\n",
    "                    wedge_lst.append('\\t'.join([node1,node2,str(freq)]))\n",
    "            else:\n",
    "                if freq>=lthr and freq<=hthr:\n",
    "                    wedge_lst.append('\\t'.join([node1,node2,str(freq)]))\n",
    "        return wedge_lst\n",
    "    \"\"\" extract community structures \"\"\"\n",
    "    def detect_communities(self,nodes_freq,treeid,lthr=1,hthr='infty',save2file='False',draw='False'):\n",
    "        SCRPT_BASE_DIR = os.path.join(RESOURCE_DIR,'scripts')\n",
    "        sys.path.append(SCRPT_BASE_DIR)\n",
    "        import community\n",
    "        import networkx as nx\n",
    "        nodes_lst = nodes_freq.keys()\n",
    "        wedge_lst = self.conv2edge_weight_list(nodes_freq,lthr=lthr,hthr=hthr)\n",
    "        G = nx.parse_edgelist(wedge_lst,nodetype=str, data=(('weight',int),))\n",
    "        partition = community.best_partition(G) #first compute the best partition\n",
    "        ##import louvain\n",
    "        ##import igraph\n",
    "        ##snodes = set()\n",
    "        ##for key in nodes_lst:\n",
    "        ##    item1,item2 = key.split('__')\n",
    "        ##    snodes.add(item1)\n",
    "        ##    snodes.add(item2)\n",
    "        ##nodes = list(snodes)\n",
    "        ##edges = []\n",
    "        ##for item in wedge_lst:\n",
    "        ##    n1,n2,freq = item.split('\\t')\n",
    "        ##    n1idx = nodes.index(n1)\n",
    "        ##    n2idx = nodes.index(n2)\n",
    "        ##    edges.append((n1idx,n2idx))\n",
    "        ##G = igraph.Graph(vertex_attrs={\"label\": nodes},edges=edges)\n",
    "        ##partition = louvain.find_partition(G);\n",
    "        ## get community-network ##\n",
    "        baseid = treeid\n",
    "        for com in range(0,len(set(partition.values()))):\n",
    "            wlst = [node for node in partition.keys() if partition[node]==com]\n",
    "            sub_nodes_freq = {}\n",
    "            for word in wlst:\n",
    "                hitnodes = [nodes for nodes in nodes_lst if word in nodes]\n",
    "                for hitnode in hitnodes:\n",
    "                    node1, node2 = hitnode.split('__')\n",
    "                    if node1 in wlst and node2 in wlst:\n",
    "                        sub_nodes_freq[hitnode] = nodes_freq[hitnode]\n",
    "            treeid = baseid+'d'+str(com)\n",
    "            save_file_name = os.path.join(self.LDA_DIR,self.header+self.source+'_com_'+treeid+'.pkl')\n",
    "            fw = open(save_file_name,'wb')\n",
    "            pickle.dump(sub_nodes_freq,fw)\n",
    "            fw.close()\n",
    "            self.conv_pickle2txt(treeid)\n",
    "        if not draw=='False':\n",
    "            import matplotlib.pyplot as plt\n",
    "            #drawing\n",
    "            size = float(len(set(partition.values())))\n",
    "            pos = nx.spring_layout(G)\n",
    "            count = 0.0\n",
    "            for com in set(partition.values()):\n",
    "                count += 1.0\n",
    "                list_nodes = [nodes for nodes in partition.keys() if partition[nodes]==com]\n",
    "                nx.draw_networkx_nodes(G,pos,list_nodes,node_size=20,node_color=str(count/size))\n",
    "            nx.draw_networkx_edges(G,pos,alpha=0.5)\n",
    "            plt.show()\n",
    "        #return len(set(partition.values()))       \n",
    "    \"\"\" get hierarchical topics with the given depth \"\"\"\n",
    "    def get_hierarchical_topics(self,maxdepth=1):\n",
    "        import glob\n",
    "        REPdepth = re.compile('d\\d+')\n",
    "        main_dir = os.getcwd()\n",
    "        for d in range(0,maxdepth):\n",
    "            os.chdir(self.LDA_DIR)\n",
    "            com_lst = glob.glob(self.header+self.source+'_com_*.pkl')\n",
    "            depth_lst = []\n",
    "            for item in com_lst:\n",
    "                item = item.replace(self.header+self.source+'_com_','')\n",
    "                treeid = item.replace('.pkl','')\n",
    "                lst = REPdepth.findall(treeid)\n",
    "                if len(lst)==d+1:\n",
    "                    depth_lst.append(treeid)   \n",
    "            os.chdir(main_dir)\n",
    "            for treeid in depth_lst:\n",
    "                read_file_name = self.header+self.source+'_com_'+treeid+'.pkl'\n",
    "                pf = open(os.path.join(self.LDA_DIR,read_file_name),'rb')\n",
    "                nodes_freq = pickle.load(pf)\n",
    "                self.detect_communities(nodes_freq,treeid,lthr=1,hthr='infty',save2file='True',draw='False')\n",
    "    \"\"\" print a list of words for each tree-branch \"\"\"\n",
    "    def get_topicwords(self,treeid):\n",
    "        pf =  open(os.path.join(self.LDA_DIR,self.header+self.source+'_com_'+treeid+'.pkl'),'rb')\n",
    "        nodes_freq = pickle.load(pf)\n",
    "        key_lst = nodes_freq.keys()\n",
    "        wlst = []\n",
    "        for key in key_lst:\n",
    "            node1, node2 = key.split('__')\n",
    "            wlst.append(node1)\n",
    "            wlst.append(node2)\n",
    "        wlst = list(set(wlst))\n",
    "        wlst.sort()\n",
    "        return wlst\n",
    "    \"\"\" get all tree information \"\"\"\n",
    "    def get_all_topicwords(self):\n",
    "        import glob\n",
    "        REPdn = re.compile('d\\d+')\n",
    "        main_dir = os.getcwd()\n",
    "        os.chdir(self.LDA_DIR)\n",
    "        com_lst = glob.glob(self.header+self.source+'_com_*.pkl')\n",
    "        root_idx = com_lst.index(self.header+self.source+'_com_d0.pkl')\n",
    "        com_lst.pop(root_idx)\n",
    "        com_lst.sort()\n",
    "        fw = open(os.path.join(self.LDA_DIR,self.header+self.source+'_comwise_words.txt'),'w')\n",
    "        for item in com_lst:\n",
    "            item = item.replace(self.header+self.source+'_com_','')\n",
    "            treeid = item.replace('.pkl','')\n",
    "            fw.write(treeid+'\\n')\n",
    "            wlst = self.get_topicwords(treeid)\n",
    "            fw.write(' '.join(wlst)+'\\n')\n",
    "        fw.close()\n",
    "        dns_set = set()\n",
    "        for item in com_lst:\n",
    "            item = item.replace(self.header+self.source+'_com_','')\n",
    "            treeid = item.replace('.pkl','')\n",
    "            dns = REPdn.findall(treeid)\n",
    "            dns_set.add(len(dns))\n",
    "        maxdepth = max(list(dns_set))\n",
    "        fw = open(os.path.join(self.LDA_DIR+'/'+self.header+'_maxdepth_words.txt'),'w')\n",
    "        for item in com_lst:\n",
    "            item = item.replace(self.header+self.source+'_com_','')\n",
    "            treeid = item.replace('.pkl','')\n",
    "            dns = REPdn.findall(treeid)\n",
    "            if len(dns)==maxdepth:\n",
    "                fw.write(treeid+'\\n')\n",
    "                wlst = self.get_topicwords(treeid)\n",
    "                fw.write(' '.join(wlst)+'\\n')\n",
    "        fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class make_cattree():\n",
    "    def __init__(self,source='abstractL'):\n",
    "        self.source = source\n",
    "        self.REPcat = re.compile('(d\\d{1,2})+$')\n",
    "        self.REP1stline = re.compile('all_stemcat\\[d0\\]')\n",
    "        self.LDA_DIR = os.path.join(MAIN_DIR,'LDA')\n",
    "        self.id_name = {}\n",
    "    def generate_space(self,depth=3):\n",
    "        sp = ''\n",
    "        for i in range(4*(depth-1)):\n",
    "            sp = sp+' '\n",
    "        return sp\n",
    "    def generate_lowest_part(self,depth=3):\n",
    "        s = ''\n",
    "        sp = self.generate_space(depth)\n",
    "        tmp = 'depth%d = all_stemcat[d%d]\\n' % (depth,depth-1)\n",
    "        s = s+sp+tmp\n",
    "        tmp = 'depth%d.sort()\\n' % (depth)\n",
    "        s = s+sp+tmp\n",
    "        tmp = 'for d%d in depth%d:\\n' % (depth,depth)\n",
    "        s = s+sp+tmp\n",
    "        tmp = '    fw.write(cat_words[d%d])\\n' % (depth)\n",
    "        s = s+sp+tmp\n",
    "        return s\n",
    "    def create_recursive_tree(self,depth):\n",
    "        html_file_name = os.path.join(self.LDA_DIR,self.source)\n",
    "        body = 'fw = open(\\'%s_tree.html\\',\\'w\\')\\n' % (html_file_name)\n",
    "        for i in range(1,depth):\n",
    "            s = ''\n",
    "            sp = self.generate_space(i)\n",
    "            tmp = 'depth%d = all_stemcat[d%d]\\n' % (i,i-1)\n",
    "            s = s+sp+tmp\n",
    "            tmp = 'depth%d.sort()\\n' % (i)\n",
    "            s = s+sp+tmp\n",
    "            tmp = 'for d%d in depth%d:\\n' % (i,i)\n",
    "            s = s+sp+tmp\n",
    "            tmp = '    try:\\n'\n",
    "            s = s+sp+tmp\n",
    "            tmp = '        name = id_name[d%d]\\n' % (i)\n",
    "            s = s+sp+tmp\n",
    "            tmp = '    except:\\n'\n",
    "            s = s+sp+tmp\n",
    "            tmp = '        name = d%d\\n' % (i)\n",
    "            s = s+sp+tmp\n",
    "            #tmp = '    s = \\'<li><span class=\\\"topic%d\\\">\\%s</span>\\n\\' \\% (name)' % (i)\n",
    "            tmp = '    s = \\'<li><span class=\"topic%d\">%ss</span>\\\\n\\' %s (name)\\n' % (i,\"%\",\"%\")\n",
    "            s = s+sp+tmp\n",
    "            tmp = '    fw.write(s)\\n'\n",
    "            s = s+sp+tmp\n",
    "            tmp = '    fw.write(\\'<ul>\\\\n\\')\\n'\n",
    "            s = s+sp+tmp\n",
    "            body = body+s\n",
    "        s = self.generate_lowest_part(depth)\n",
    "        body = body+s\n",
    "        tail = ''\n",
    "        for i in range(1,depth):\n",
    "            sp = self.generate_space(depth-i+1)\n",
    "            tmp = 'fw.write(\\'</ul></li>\\\\n\\')\\n'\n",
    "            tail = tail+sp+tmp\n",
    "        body = body+tail+'fw.close()\\n'\n",
    "        body = self.REP1stline.sub('[stem for stem in all_stemcat.keys() if get_depth(stem)==1]',body)\n",
    "        return body\n",
    "    def create_exec_script(self,depth):\n",
    "        body = self.create_recursive_tree(depth)\n",
    "        fr = open(os.path.join(WS_DIR,'make_cattree_head.py')).read()\n",
    "        fw = open('make_cattree.py','w')\n",
    "        fw.write(fr+body)\n",
    "        fw.close()\n",
    "    def create_tree_html(self,depth):\n",
    "        self.create_exec_script(depth)\n",
    "        cmd = 'python make_cattree.py %s %s %s' % (THEME,self.LDA_DIR,depth)\n",
    "        #print(cmd)\n",
    "        os.system(cmd)\n",
    "    def concatenate_html(self,jp='True'):\n",
    "        template_dir = os.path.join(WS_DIR,'template/')\n",
    "        head_file_name = os.path.join(template_dir,'head.html')\n",
    "        head = open(head_file_name).read()\n",
    "        body_file_name = os.path.join(self.LDA_DIR,self.source+'_tree.html')\n",
    "        body = open(body_file_name).read()\n",
    "        tail_file_name = os.path.join(template_dir,'tail.html')\n",
    "        tail = open(tail_file_name).read()\n",
    "        fw = open(body_file_name,'w')\n",
    "        fw.write(head+'\\n'+body+'\\n'+tail)\n",
    "        fw.close()\n",
    "        if jp=='True':\n",
    "            template_dir = os.path.join(WS_DIR,'template')\n",
    "            head_file_name = os.path.join(template_dir,'literature_html_headJP.html')\n",
    "            head = open(head_file_name).read()\n",
    "            body_file_name = os.path.join(self.LDA_DIR,self.source+'_treeJP.html')\n",
    "            body = open(body_file_name).read()\n",
    "            tail_file_name = os.path.join(template_dir,'literature_html_tail.html')\n",
    "            tail = open(tail_file_name).read()\n",
    "            fw = open(body_file_name,'w')\n",
    "            fw.write(head+'\\n'+body+'\\n'+tail)\n",
    "            fw.close()\n",
    "    def conv2jp(self):\n",
    "        ds = DictSearch()\n",
    "        read_file_name = os.path.join(self.LDA_DIR,self.source+'_tree.html')\n",
    "        fr = open(read_file_name).readlines()\n",
    "        save_file_name = os.path.join(self.LDA_DIR,self.source+'_treeJP.html')\n",
    "        fw = open(save_file_name,'w')\n",
    "        for line in fr:\n",
    "            tmp = self.REPlihead.sub('',line)\n",
    "            lst = self.REPareftail.split(tmp)\n",
    "            for item in lst:\n",
    "                if self.REParefhead.search(item):\n",
    "                    word = self.REParefhead.sub('',item)\n",
    "                    word_pair = ds.translate2jp(word)\n",
    "                    if word_pair[1]=='Not Found':\n",
    "                        word_pair = ds.translate2jp(word.lower())\n",
    "                    if not word_pair[1]=='Not Found':\n",
    "                        jp_rep_word = word_pair[1].split(', ')[0]\n",
    "                        rm_lst = self.REPkakko.findall(jp_rep_word)\n",
    "                        for rm in rm_lst:\n",
    "                            jp_rep_word = jp_rep_word.replace(rm,'')\n",
    "                        line = line.replace(word_pair[0],jp_rep_word+' ['+word_pair[0]+']')\n",
    "            fw.write(line.encode('utf-8'))\n",
    "        fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/petadimensionlab/ws/apps/refminer/retriever /Users/petadimensionlab/ws/apps/refminer/resources/FilReter/db/dict/PIDVocab/PIDVocab.db 0.7 abcdefg\n",
      "/Users/petadimensionlab/ws/apps/refminer/retriever /Users/petadimensionlab/ws/apps/refminer/resources/FilReter/db/dict/PIDVocab/PIDVocab.db 0.7 abcdefg\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/petadimensionlab/ws/apps/refminer/PID/LDA/PIDabstractL_state.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-1228b7b3d30a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m#print( cmd )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mlda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLDA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'abstractL'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_LDA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_hierarchical_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_topicwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-1515993c5e12>\u001b[0m in \u001b[0;36mseq_LDA\u001b[0;34m(self, iter, convergence_check)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mnodes_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexec_LDA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0mnodes_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_nodes_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnodes_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msave2file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'True'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconvergence_check\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'False'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-1515993c5e12>\u001b[0m in \u001b[0;36mexec_LDA\u001b[0;34m(self, num, parms)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mlst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mpmid_lst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mostate_zip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mfile_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/gzip.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename, mode, compresslevel, encoding, errors, newline)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mgz_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mbinary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"write\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mbinary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/gzip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmyfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/petadimensionlab/ws/apps/refminer/PID/LDA/PIDabstractL_state.gz'"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    \"\"\" RefMiner \"\"\"\n",
    "    #pm = Pubmed(THEME)\n",
    "    \"\"\" full text search (create db) \"\"\"\n",
    "    ais = AbstractIndexSearch(MAIN_DIR,'abstractL')\n",
    "    ais.lemmatize_docs()\n",
    "    adis = IndexSearch(MAIN_DIR,'abstractL')\n",
    "    adis.create_input_textfile(input_file=os.path.join(MAIN_DIR,\"abstractL_input.txt\"))\n",
    "    \"\"\" LDA \"\"\"\n",
    "    ## Error handling while performing mallet: certain machine symbols may cause error. see below: \n",
    "    ## https://stackoverflow.com/questions/21417809/error-while-importing-txt-file-into-mallet##\n",
    "    cmd = 'mv %s/abstractL_input.txt %s/abstractL_input_original.txt' % (MAIN_DIR,MAIN_DIR)\n",
    "    os.system(cmd)\n",
    "    #print( cmd )\n",
    "    cmd = 'tr -dc [:alnum:][\\ ,.]\\\\n < %s/abstractL_input_original.txt > %s/abstractL_input.txt' % (MAIN_DIR,MAIN_DIR)\n",
    "    os.system(cmd)\n",
    "    #print( cmd )\n",
    "    lda = LDA(source='abstractL')\n",
    "    lda.seq_LDA(iter=1)\n",
    "    lda.get_hierarchical_topics(maxdepth=4)\n",
    "    lda.get_all_topicwords()\n",
    "    \"\"\" Tree structure \"\"\"\n",
    "    tree = make_cattree(source='abstractL')\n",
    "    tree.create_tree_html(depth=4)\n",
    "    tree.concatenate_html(jp='False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
